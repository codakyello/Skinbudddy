Currently; We store session chat history, manage our own rolling sumaries etc and build it on every request if sessionId is present or we create a new sessionId and pass it to the client if it dosent exist
What should happen; I want OpenAi to manage everything by providing us with the previous_chat_id. It will just be like how sessionId works today, when we hit up the api without a previous_chat_id we dont include it, we wait for a response from openApi get the previous_chat_id, send to the frontend and then on subsequent request, we keep sending it back to keep track of the conversation. We let openAi manage all the conversation state and rolling summary etc

Currently; We use a loop to check to run the modal, loop through toolOutputs, append the result to a list of toolOutputs, then fit it back into the modal in the next loop turn and then when there are no more toolOutputs, we wait for the modal final reply by calling the model yet again.
What should happen; Everything should be done in one model call/lifecycle, when a tool is needed, an event should be emitted, and we should call the relevant methods to handle the event. So every toolcall, and generating final response or reply should happen within one model call. The model should only send out a final reply. After the tool calls, the same way we are extracting products from the tool calls, calling another model to refine products should not change, what should change is how the model returns the refined product, today we are calling a initiate a call for a tool but not actually calling it we are just using the args of the tool call provided by the model as the result to the refined products, this can be done in a better way by using specifying a return type in the modal call (not sure how this works yet), for routine how we are calling another llm api in the recommend function also should not change the only thing that should change is that we should pass in the userPrompt to recommenend function so the llm can see what the user is asking for and provide an answer tailored to the users prompt.
