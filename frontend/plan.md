Currently; We use a loop to run the modal, loop through toolOutputs, append the result to a list of toolOutputs, then run it back into the model in the next loop turn and then when there are no more toolOutputs, we wait for the modal final reply by calling the model yet again.

What should happen; Everything should be done in one model call/lifecycle, when a tool is needed, an event should be emitted, and we should call the relevant methods to handle the event. So every toolcall, and generating final response or reply should happen within one model call. The model should only send out a final reply. After the tool calls, the same way we are extracting products from the tool calls, calling another model to refine products should not change, what should change is how the model returns the refined product, today we are calling a initiate a call for a tool but not actually calling it we are just using the args of the tool call provided by the model as the result to the refined products, this can be done in a better way by using specifying a return type in the modal call (not sure how this works yet), for routine how we are calling another llm api in the recommend function also should not change the only thing that should change is that we should pass in the userPrompt to recommenend function so the llm can see what the user is asking for and provide a response tailored to the users prompt.
